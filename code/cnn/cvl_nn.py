"""
Function：Convolution Neural Network
Author：lzb
Date：2021.01.10
"""

import numpy as np
import operator

from nn.neural_network import NeuralNetwork
from gl import errorcode
from gl.common_enum import ArrayDim
from gl.common_function import rand_array_3

from cnn.convolution import Convolution, ConvolutionType, Reversal, cal_cvl_wh

"""
class：CVLNeuralNetwork，卷积神经网络
说明：
1、继承自 NeuralNetwork
2、重载 _modify_wb 函数

特别说明：
1、每一层的 w，还是一个 matrix，不过它的定义是：卷积核
2、每一层的神经元个数，将由上一层的神经元个数和卷积参数确定（w，s，padding），而不是由外部输入
3、卷积参数：步长 s = 1，补零 padding = 0
4、激活函数是 ReLU 函数
5、输入样本 sx，为了计算方便，将是1个3维矩阵，为了方便理解，可以这么认为：
A. 第1维：图像像素的 x 坐标
B. 第2维：图像像素的 y 坐标
C. 第3维：图像像素的颜色值。如果是 RGB 图像，则第3维有3个值（r, g, b），如果是灰度图像，则第3维只有1个值（gray）
6、从这个意义上讲，参数 sx_dim 将变为1个”3元祖“：sx_dim[0] 表示图像的宽度，sx_dim[1] 表示图像的高度，sx_dim[2] 表示图像的颜色深度
7、输出样本，暂时还定义为分类，所以它还是一个1维向量。因此，sy_dim 还是表示1维向量的元素个数（1维向量的维度）
"""


class CVLNeuralNetwork(NeuralNetwork):
    # 可以理解为图像宽度
    width = 1

    # 可以理解为图像高度
    height = 1

    # 可以理解为颜色深度（颜色维度）
    depth = 1

    # 卷积对象
    cvl = None

    # 每一层神经网络的输出（经过激活函数以后的输出），a 是一个三维数组
    a_list = None

    # 卷积步长
    s = 1

    # 卷积两端补齐长度
    padding = 0

    # 卷积类型
    cvl_type = ConvolutionType.Narrow

    # 是否翻转卷积
    rev = Reversal.NO_REV

    """
    功能：构造函数
    参数：
    cvl：卷积对象
    返回值：NULL
    """

    def __init__(self, cvl):
        self.cvl = cvl

    """
    功能：参数校验
    参数：NULL    
    返回值：错误码    
    """

    def _valid(self):
        # 调用父类的 _valid
        err = super()._valid()

        if errorcode.SUCCESS != err:
            return err

        # 校验 w_shape_list

        if self.w_shape_list is None:
            return errorcode.FAILED

        if 0 >= len(self.w_shape_list):
            return errorcode

        # 这里只处理3维数组
        shape = self.w_shape_list[0]

        if 3 != len(shape):
            return errorcode.FAILED

        if (0 >= shape[0]) or (0 >= shape[1]) or (0 >= shape[1]):
            return errorcode.FAILED

        return errorcode.SUCCESS

    """
    功能：校验每层神经元
    参数：NULL    
    返回值：错误码
    说明：对于卷积神经网络来说，这里不需要校验
    """

    def _valid_layer_neuron(self):
        return errorcode.SUCCESS

    """
    功能：校验样本
    参数：NULL    
    返回值：错误码
    说明：理解卷积神经网络，理解卷积对图像的处理，恐怕是从样本的校验开始
    """

    def _valid_sample(self):
        # 1 输入样本的数量与输出样本的数量，须相同（equal with parent class）
        len1 = len(self.sx_list)
        len2 = len(self.sy_list)

        if len1 != len2:
            return errorcode.FAILED

        # 2 样本数量，须 >= 1（same as parent class）
        sample_count = len(self.sx_list)
        if 1 > sample_count:
            return errorcode.FAILED

        # 3. 样本数组维度（different with parent class）

        # 3.1 输入数组维度
        sx_dim = self.sx_list[0].shape

        # 输入样本必须是3维样本：图像的宽度、高度，颜色的深度
        if ArrayDim.THREE.value != len(sx_dim):
            return errorcode.FAILED

        width = sx_dim[0]
        height = sx_dim[1]
        depth = sx_dim[2]

        # 图像宽度/高度须大于0
        if (0 > width) or (0 > height):
            return errorcode.FAILED

        # 颜色深度须介于1~3之间
        if (3 < depth) or (1 > depth):
            return errorcode.FAILED

        # 3.2 每一个输入/输出样本的维度
        for i in range(0, sample_count):
            shape_in = self.sx_list[i].shape
            shape_out = self.sy_list[i].shape

            # 输入样本的维度须相等(都是3维)
            if not operator.eq(sx_dim, shape_in):
                return errorcode.FAILED

            # 输入样本的宽度、高度、深度
            if (shape_in[0] != width) or (shape_in[1] != height) or (shape_in[2] != depth):
                return errorcode.FAILED

            """
            # 输出样本的向量维度
            if shape_out[0] != sy_dim:
                return errorcode.FAILED

            # 输出样本只能有1列（因为是个向量）
            if shape_out[1] != 1:
                return errorcode.FAILED
            """

        return errorcode.SUCCESS

    """
    功能：初始化其它参数
    参数：NULL   
    返回值：错误码
    """

    def _init_other_para(self):
        # 样本数量
        self.sample_count = len(self.sx_list)

        # 神经网络输入，维度(3维)
        self.sx_dim = self.sx_list[0].shape

        # 图像宽度，高度，深度
        self.width = self.sx_dim[0]
        self.height = self.sx_dim[1]
        self.depth = self.sx_dim[2]

        # 神经网络输出，向量维度
        # self.sy_dim = self.neuron_count_list[self.layer_count - 1]

        # 初始化 self.layer_count
        self.layer_count = len(self.w_shape_list)

        # 初始化 W, B
        self._init_w_b()

        return errorcode.SUCCESS

    """
    功能：初始化 W, B
    参数：NULL
    返回值：NULL
    
    特别说明，这里就假设 w 是 3维数组
    """

    def _init_w_b(self):
        # 每一层 w、B 参数，w 是个 matrix，b 是个 vector（数据类型也是一个 matrix）
        self.W = list()
        self.B = list()

        # 第1层

        w0 = 1000 * np.asarray([[[3710.669733], [4652.961025], [1095.952073]],
                        [[4321.051033], [4428.459554], [1973.079748]],
                        [[6070.385994], [5199.325586], [4024.355601]]])

        b0 = 100 * np.asarray([[[0.481016], [1.479405], [1.644163], [1.664827], [2.603248], [-0.692180], [-22.570521], [5.984156], [-7.046535], [1.147998], [-6.040013], [1.435000], [1.840228], [1.881080], [1.723612], [2.414390], [1.436876], [0.501127]],
                        [[1.176733], [1.683005], [2.713023], [8.078468], [-5.992471], [-9.225040], [-21.505594], [-19.517236], [-28.218400], [-21.484858], [-14.649296], [3.574757], [3.807217], [0.847170], [1.919800], [2.341127], [1.645516], [0.663986]],
                        [[0.259214], [1.954408], [3.727751], [0.586368], [-5.346028], [-24.099575], [-25.984599], [22.533014], [34.946095], [8.799916], [-37.668836], [-47.373917], [10.751742], [-0.999125], [3.709080], [4.193422], [3.192237], [1.583705]],
                        [[0.183992], [2.145967], [3.590236], [3.791182], [-0.654658], [-15.513380], [2.456422], [-26.862966], [-6.094008], [-15.857716], [-14.187334], [2.315549], [31.614812], [-4.578117], [4.093729], [3.836288], [3.429718], [1.717084]],
                        [[0.638994], [1.823400], [4.084786], [-5.156160], [7.841026], [-17.522750], [33.872747], [8.977444], [-1.237709], [8.503539], [-12.446968], [-34.233941], [-15.249219], [16.398821], [4.074916], [4.172607], [3.176907], [1.465232]],
                        [[0.772551], [1.514471], [3.269107], [-1.923336], [5.030123], [-7.654132], [2.850638], [-58.421646], [9.391303], [55.564308], [-37.230079], [-22.807281], [-6.208074], [26.062485], [4.086080], [3.878051], [3.480432], [2.298993]],
                        [[0.671210], [2.367788], [3.492589], [-1.598956], [-0.902322], [1.848229], [-26.826901], [-1.991831], [43.175759], [-70.145489], [-10.499420], [-7.633727], [0.263590], [6.551044], [3.647208], [4.104495], [3.385511], [1.307969]],
                        [[0.483535], [2.469480], [-5.019282], [1.821629], [-42.371161], [5.304859], [-37.254061], [77.663842], [-79.752569], [-28.765551], [30.481695], [-12.088809], [-3.246721], [6.314126], [3.321091], [4.002362], [3.347963], [2.211049]],
                        [[0.166867], [2.048198], [-2.116968], [-18.149841], [-14.871182], [16.187640], [-15.197987], [-35.576005], [-37.221616], [70.778967], [-22.585206], [-2.175218], [-14.632462], [7.564049], [2.928270], [3.279351], [3.186351], [1.758367]],
                        [[0.851498], [2.397779], [7.018646], [-19.953396], [13.087715], [-29.407327], [15.101422], [-23.442321], [68.109863], [-71.255025], [3.186463], [7.628696], [6.962931], [9.199756], [3.612293], [3.950190], [2.709571], [1.681855]],
                        [[0.986109], [2.464037], [8.773463], [-19.538770], [-16.806587], [-9.554985], [7.013863], [-8.931838], [19.987893], [3.676565], [37.854401], [-35.194586], [13.520102], [-1.354770], [3.764206], [3.040560], [2.774600], [1.403052]],
                        [[0.682418], [2.108791], [22.649892], [-35.662664], [-29.555124], [2.996384], [6.215033], [12.358918], [-14.108391], [21.496091], [-17.624577], [-9.619745], [19.351015], [4.453224], [3.409606], [3.699201], [3.557813], [1.434591]],
                        [[0.214433], [2.426809], [-1.710304], [1.299383], [-33.250223], [37.516076], [-26.690552], [-0.477240], [8.400167], [15.684608], [-29.466649], [5.582492], [-6.672823], [4.438397], [3.593108], [3.834974], [3.068126], [2.192989]],
                        [[0.643346], [1.858464], [0.361328], [11.792930], [-85.045735], [-0.716241], [-19.332720], [57.348995], [-36.080911], [-15.898030], [10.880838], [-0.607350], [-10.553449], [3.856304], [2.972978], [3.779078], [3.297318], [1.845069]],
                        [[0.418171], [1.749720], [3.989544], [4.382302], [5.080403], [-13.058205], [1.339263], [-39.167502], [-0.118310], [21.876050], [-17.792757], [-11.404957], [-7.822473], [5.977874], [3.639743], [3.872232], [3.098578], [1.366064]],
                        [[0.419107], [1.933545], [3.337127], [2.684315], [7.545824], [-35.294587], [33.162389], [-6.933339], [19.535522], [-37.014542], [-2.076363], [18.930902], [2.205434], [8.094306], [3.418088], [3.447455], [3.601249], [1.894300]],
                        [[0.079009], [0.286546], [1.022037], [2.107463], [-2.273010], [-0.649206], [13.136886], [-25.728171], [16.058342], [7.177684], [1.757698], [1.806189], [1.633105], [1.535858], [1.864792], [2.131091], [1.098139], [0.491458]],
                        [[-0.219509], [0.510614], [1.868633], [1.169649], [1.519331], [6.453987], [-2.939635], [-14.354317], [8.249411], [1.649194], [1.061232], [0.763301], [0.399078], [1.371210], [1.753548], [1.156977], [1.292655], [0.534007]]])

        self.W.append(w0)
        self.B.append(b0)

        # 第2层

        w1 = 100 * np.asarray([[[0.783723], [0.313253], [-0.078143]],
                        [[0.051495], [0.091621], [0.300788]],
                        [[0.521301], [0.720201], [0.216941]]])

        b1 = 100 * np.asarray([[[1.672836], [1.679473], [1.695475], [1.676572], [1.669898], [-1.474858], [-2.073087], [-1.474858], [-1.474858], [-0.685062], [1.667057], [1.664220], [1.710309], [1.678113], [1.685421], [1.687634]],
                        [[1.665277], [1.662224], [1.705135], [-0.705761], [-1.484957], [-2.546582], [-2.104846], [-2.118393], [-2.985055], [-2.986434], [-2.118393], [-0.685847], [1.680542], [1.680137], [1.698668], [1.668004]],
                        [[1.688326], [1.680050], [1.674775], [1.668446], [-0.578536], [-1.553646], [-2.593382], [-0.949328], [-1.704736], [-2.564164], [-1.589217], [-1.581363], [1.673129], [1.710729], [1.669800], [1.672070]],
                        [[1.703128], [1.679874], [1.669394], [1.684927], [-0.756516], [-2.133169], [-2.181063], [-0.756516], [-2.985723], [-2.984103], [-0.685847], [-1.581363], [1.667327], [1.680501], [1.685006], [1.665938]],
                        [[1.678127], [1.668468], [1.672047], [-0.756516], [-1.590971], [-2.133169], [-1.671096], [-3.401474], [-3.436188], [-2.160564], [1.704427], [-0.820700], [1.678673], [1.684171], [1.664198], [1.685935]],
                        [[1.692954], [1.687184], [1.682789], [-0.756516], [-1.617018], [-0.685847], [-2.988908], [-3.904535], [-2.056386], [-1.637888], [1.663832], [-0.820700], [1.664582], [1.683815], [1.662592], [1.664087]],
                        [[1.681076], [1.671952], [1.676067], [-0.756516], [-1.617018], [-1.558727], [-2.584650], [-2.562646], [-0.949328], [-0.949328], [-1.580490], [-0.820700], [1.675133], [1.711613], [1.664483], [1.698822]],
                        [[1.661863], [1.712461], [-0.756516], [-0.820700], [-2.586081], [-2.993921], [-3.400234], [-2.982620], [-2.156551], [-2.156551], [-2.127785], [-0.756516], [1.699766], [1.687354], [1.702722], [1.667459]],
                        [[1.683487], [1.693318], [-0.756516], [-1.609175], [-2.568773], [-2.062288], [-2.114036], [-2.133385], [-0.748720], [-1.638745], [-2.113091], [1.686274], [1.671893], [1.712806], [1.707280], [1.691251]],
                        [[1.667499], [1.663479], [1.665806], [-1.581363], [-2.103450], [-0.578536], [-0.869987], [-1.590971], [-2.172507], [-2.179975], [-2.113091], [-0.685847], [1.683019], [1.673716], [1.665797], [1.682961]],
                        [[1.684071], [1.678191], [-0.685062], [-2.118393], [-2.077334], [1.664176], [-1.629923], [-0.756516], [-2.172507], [-0.820700], [-0.685062], [-0.685847], [1.684827], [1.702270], [1.688195], [1.682184]],
                        [[1.668261], [1.672674], [-1.561334], [-2.986434], [-1.504981], [-0.748720], [-1.629923], [-2.600523], [-2.600523], [1.670346], [-0.685062], [-0.685847], [1.683488], [1.699618], [1.674943], [1.692335]],
                        [[1.678318], [1.678561], [-0.775778], [-3.426320], [-2.113091], [-2.582876], [-2.589941], [-2.600523], [-0.685062], [-1.522185], [-1.522185], [-0.685847], [1.687363], [1.691047], [1.684109], [1.668603]],
                        [[1.694579], [1.667069], [1.695423], [-1.517566], [-2.560388], [-3.406595], [-2.982190], [-2.118078], [-2.056386], [-1.475743], [-0.578536], [-0.578536], [1.711207], [1.705663], [1.668635], [1.692273]],
                        [[1.692548], [1.688322], [1.666762], [1.684846], [1.678964], [-0.869987], [-1.553646], [-0.685847], [1.669606], [1.666620], [1.710338], [1.698657], [1.706056], [1.663717], [1.673198], [1.694950]],
                        [[1.687191], [1.661793], [1.708868], [1.691789], [-0.869987], [-0.869987], [-0.756516], [1.674389], [1.664361], [1.693832], [1.694665], [1.708483], [1.663648], [1.670192], [1.688745], [1.671990]]])

        self.W.append(w1)
        self.B.append(b1)

        """
        # 1. W，B 是 list
        self.W = list()
        self.B = list()

        # 2. 针对每一层进行初始化
        b = 0
        for layer in range(0, self.layer_count):
            # 2.1 每一层的卷积核
            width = self.w_shape_list[layer][0]
            height = self.w_shape_list[layer][1]

            # 如果是第一层，depth = 输入层的 depth
            if 0 == layer:
                depth = self.w_shape_list[layer][2]
            # 否则的话，depth = 1
            else:
                depth = 1

            w = rand_array_3(width, height, depth)
            # w = np.zeros([width, height, depth])
            self.W.append(w)

            # 2.2 每一层的 b

            # 如果是第一层，x 就是样本输入
            if 0 == layer:
                x = self.sx_list[0]
            # 否则的话，x 是上一层的输出
            # 上一层的输出的 width，height 等同于 b
            else:
                x = b

            width, height = cal_cvl_wh(w, x, self.s)

            # 每一层的b，都是 [width, height, depth] 3维数组
            depth = 1  # b 的 depth = 1
            b = rand_array_3(width, height, depth)
            # b = np.zeros([width, height, depth])

            self.B.append(b)
        """

    """
    功能：计算某一层神经网络的输出
    参数：
    x：该层神经网络的输入，x 是一个3维数组
    w: 该层神经网络的 w 参数, w 是一个3维数组
    b：该层神经网络的 b 参数，b 是一个2维数组
    返回值：y，该层神经网络的输出（sigmoid(cvl(w, x) + b)）， y 是一个3维数字
    """

    def _calc_layer(self, x, layer):
        # 1、获取该层的参数：w, b
        w = self.W[layer]
        b = self.B[layer]

        # 2、计算卷积结果
        y, err = self.cvl.convolution_sum_depth(w, x)
        # y, err = cvl.convolution(w, x)

        # 3. y = y + b
        y_width = y.shape[0]
        y_height = y.shape[1]
        y_depth = y.shape[2]

        for i in range(0, y_width):
            for j in range(0, y_height):
                for k in range(0, y_depth):
                    y[i, j, k] += b[i, j, k]

        # 针对每一个元素，调用激活函数
        for i in range(0, y_width):
            for j in range(0, y_height):
                for k in range(0, y_depth):
                    y[i, j, k] = self.activation.active(y[i, j, k])

        return y

    """
    功能：修正 W，B
    参数：
    nn_y_list：神经网路计算的每一层结果，nn_y 是一个向量
    sx：训练样本的输入，sx 是一个向量
    sy：训练样本的输出，sy 是一个向量 
    返回值：NULL
    """

    def _modify_wb(self, nn_y_list, sx, sy):
        # 1. 后向传播，计算 ksi_list
        ksi_list = self.__bp(nn_y_list, sy)

        # 2. 通过 ksi_list，修正 W，B
        self.__modify_wb_by_ksi_list(ksi_list, sx, nn_y_list)

    """
    功能：后向传播，计算 ksi_list
    参数：
    nn_y_list：神经网路计算的每一层结果，nn_y 是一个3维数组    
    sy：训练样本的输出，sy 是一个3维数组
    返回值：ksi_list
    说明：
    1、ksi(代表希腊字母，音：科赛)，是一个3维数组，每层都有，代表目标函数 E 对每一层中间输出的偏导
    2、ksi_list 记录每一层的 ksi
    """

    def __bp(self, nn_y_list, sy):
        # 1. 初始化 ksi_list
        ksi_list = [0] * self.layer_count

        # 2. 计算最后一层 ksi

        # 2.1 计算误差(err)：最后一层的计算结果与样本输出结果的比较（计算结果 - 训练样本的输出）
        nn_y_last = nn_y_list[self.layer_count - 1]
        err = np.subtract(nn_y_last, sy)  # 不知道3维数组是否可以这样相减

        # 2.2 计算最后一层 ksi

        # 最后一层 ksi：ksi_last，ksi_last 是个[width, height, 1] 3维数组
        width = nn_y_last.shape[0]
        height = nn_y_last.shape[1]
        depth = nn_y_last.shape[2]  # 实际的值，depth = 1

        ksi_last = np.zeros([width, height, depth])

        # 计算 ksi_last 每一个元素
        for k in range(0, depth):
            for i in range(0, width):
                for j in range(0, height):
                    ksi_last[i, j, k] = err[i, j, k] * self.activation.derivative(nn_y_last[i, j, k])

        # 将 ksi_last 放置入 ksi_list
        ksi_list[self.layer_count - 1] = ksi_last

        # 3. 反向传播，计算：倒数第2层 ~ 第1层的 ksi
        for layer in range(self.layer_count - 2, -1, -1):
            # 下一层的 ksi
            ksi_next = ksi_list[layer + 1]

            # 下一层的 w
            w = self.W[layer + 1]

            # 当前层的 ksi
            ksi_cur, err = self.cvl.convolution(w, ksi_next, Reversal.REV, ConvolutionType.Wide)

            # 将当前层计算出的 ksi 放置到 ksiList
            ksi_list[layer] = ksi_cur

        # return 计算结果
        return ksi_list

    """
    功能：修正 W，B
    参数： 
    ksi_list：每一层的 ksi 的列表，ksi 是一个3维数组
    sx：输入样本，sx 是一个3维数组
    nn_y_list：神经网络的每一层的计算结果列表，nn_y 是一个3维数组    
    返回值：NULL  
    """

    def __modify_wb_by_ksi_list(self, ksi_list, sx, nn_y_list):
        # 逐层修正
        for layer in range(0, self.layer_count):
            # 当前层 w, b, ksi
            w = self.W[layer]
            b = self.B[layer]
            ksi = ksi_list[layer]

            # 上一层的输入
            if 0 == layer:
                v = sx
            else:
                v = nn_y_list[layer - 1]

            # 损失函数针对当前层的 w 的偏导(partial derivative)，w_pd 是1个3维数组
            w_pd, err = self.cvl.convolution(ksi, v)

            # 修正当前层的 w
            self.W[layer] = np.subtract(w, self.rate * w_pd)  # 不知道3维数组是否可以这样相减

            # 损失函数针对当前层的 b 的偏导(partial derivative)，b_pd 等于 ksi
            b_pd = ksi

            # 修正当前层的 b
            self.B[layer] = np.subtract(b, self.rate * b_pd)  # 不知道3维数组是否可以这样相减

    """
    功能：预测
    参数：
    sx_list：待预测的样本列表，其中 sx 是向量 
    返回值：预测结果
    """

    def predict(self, sx_list, sy_list):
        count = len(sx_list)
        py_list = list()

        for i in range(0, count):
            sx = sx_list[i]
            nn_y_list = self._calc_nn(sx)

            # 最后一层的 nn_y，才是神经网络的最终输出
            nn_y = nn_y_list[len(nn_y_list) - 1]

            # 然后再添加到预测列表
            py_list.append(nn_y)

        return py_list
